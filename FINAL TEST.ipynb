{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff62c4c0-5fb4-4dc3-861b-d9df1cd71ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.feature import graycomatrix, graycoprops\n",
    "from skimage import data\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Sequential\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from keras.utils import to_categorical\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import img_to_array,array_to_img, load_img\n",
    "import shutil\n",
    "import os\n",
    "import cv2\n",
    "from tensorflow.keras.layers import Dense,Conv2D,MaxPooling2D,Flatten,Activation,Conv1D,MaxPooling1D,BatchNormalization, GlobalAveragePooling1D\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.metrics import accuracy_score,classification_report\n",
    "from tensorflow.keras.layers import Dropout,Input\n",
    "from tensorflow.keras import regularizers\n",
    "from tqdm import tqdm\n",
    "import skimage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4cf7736-a8a5-4c9e-b371-eb6dbac88488",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\PC\\anaconda3\\envs\\tf-origin\\lib\\site-packages\\keras\\src\\legacy\\preprocessing\\image.py:1047: UserWarning: This ImageDataGenerator specifies `zca_whitening`, which overrides setting of `featurewise_center`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\PC\\anaconda3\\envs\\tf-origin\\lib\\site-packages\\keras\\src\\legacy\\preprocessing\\image.py:1263: UserWarning: This ImageDataGenerator specifies `featurewise_center`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\PC\\anaconda3\\envs\\tf-origin\\lib\\site-packages\\keras\\src\\legacy\\preprocessing\\image.py:1286: UserWarning: This ImageDataGenerator specifies `zca_whitening`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    rotation_range=4,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode=\"nearest\",\n",
    "    brightness_range=(0.8, 1.2),  # Adjust brightness\n",
    "    rescale=1./255,  # Rescale pixel values\n",
    "    channel_shift_range=20.0,  # Shift color channels\n",
    "    zca_whitening=True  # Apply ZCA whitening\n",
    ")\n",
    "\n",
    "img = load_img(r\"C:\\Users\\PC\\Desktop\\PROJECT\\source6\\AK (8).jpg\")\n",
    "x = img_to_array(img)\n",
    "x = x.reshape((1,) + x.shape)\n",
    "\n",
    "i = 0\n",
    "for batch in datagen.flow(x, batch_size=1, save_to_dir=\"view\", save_prefix=\"pet\", save_format=\"jpg\"):\n",
    "    i += 1\n",
    "    if i > 20:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aa62e579-9248-475a-bbac-70cb4abb78f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_gabor_filter(image, ksize, sigma, theta, lambd, gamma):\n",
    "    kernel = cv2.getGaborKernel((ksize, ksize), sigma, theta, lambd, gamma, 0, ktype=cv2.CV_32F)\n",
    "    filtered_image = cv2.filter2D(image,cv2.CV_32F, kernel)\n",
    "    return filtered_image\n",
    "\n",
    "def extract_glcms(image):\n",
    "    image = np.uint8(image)\n",
    "    distances = [1]\n",
    "    angles = [0, np.pi/4, np.pi/2, 3*np.pi/4]\n",
    "    glcm = graycomatrix(image, distances=distances, angles=angles, symmetric=True, normed=True)\n",
    "    contrast = graycoprops(glcm, 'contrast').ravel()\n",
    "    dissimilarity = graycoprops(glcm, 'dissimilarity').ravel()\n",
    "    homogeneity = graycoprops(glcm, 'homogeneity').ravel()\n",
    "    energy = graycoprops(glcm, 'energy').ravel()\n",
    "    correlation = graycoprops(glcm, 'correlation').ravel()\n",
    "    return np.concatenate([contrast, dissimilarity, homogeneity, energy, correlation])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3196d80d-89e8-417e-93cc-447168bfb735",
   "metadata": {},
   "outputs": [],
   "source": [
    "def horizontal_projection_profile(image):\n",
    "    hpp = np.sum(image, axis=1)\n",
    "    return hpp\n",
    "\n",
    "def word_line_space_formation(hpp):\n",
    "    threshold = np.mean(hpp) * 0.1\n",
    "    space_indices = np.where(hpp < threshold)[0]\n",
    "    word_line_spaces = np.diff(space_indices)\n",
    "    return word_line_spaces\n",
    "\n",
    "def image_padding(image, target_size=(650, 650)):\n",
    "    h, w, c = image.shape\n",
    "    padded_image = np.zeros((target_size[0], target_size[1], c))\n",
    "    padded_image[:min(h, target_size[0]), :min(w, target_size[1])] = image[:min(h, target_size[0]), :min(w, target_size[1])]\n",
    "    return padded_image\n",
    "\n",
    "def block_normalization(image, block_size=(650, 650)):\n",
    "    normalized_blocks = []\n",
    "    for i in range(0, image.shape[0], block_size[0]):\n",
    "        for j in range(0, image.shape[1], block_size[1]):\n",
    "            block = image[i:i+block_size[0], j:j+block_size[1]]\n",
    "            block_mean = np.mean(block)\n",
    "            block_std = np.std(block)\n",
    "            normalized_block = (block - block_mean) / (block_std + 1e-5)\n",
    "            normalized_blocks.append(normalized_block)\n",
    "    return np.array(normalized_blocks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "944cec91-08a7-46f0-af41-758e1efc86af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 8/8 [01:46<00:00, 13.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of features: (1297, 20)\n",
      "Shape of labels: (1297,)\n",
      "Labels: ['AMBANI' 'AMBANI' 'AMBANI' ... 'TOBA' 'TOBA' 'TOBA']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def extract_features():\n",
    "    features = []\n",
    "    labels = []\n",
    "    datadir = r\"C:\\Users\\PC\\Desktop\\PROJECT\\HANDWRITING\"\n",
    "    categories = [\"AMBANI\", \"BLESSING\", \"CELESTINA\",\"ENOCH\", \"HAKEEM\", \"PETER\", \"RIDWAN\", \"TOBA\"]\n",
    "    for category in tqdm(categories):\n",
    "        path = os.path.join(datadir, category)\n",
    "        image_names = os.listdir(path)\n",
    "        for img_name in (image_names):\n",
    "            img_path = os.path.join(path, img_name)\n",
    "            img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "            img_array = np.expand_dims(img, axis=-1)\n",
    "            img_size = 850\n",
    "            new_array = cv2.resize(img_array, (img_size, img_size))\n",
    "            img_gray = cv2.adaptiveThreshold(new_array, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 5, 11)\n",
    "            bilateral_filtered = cv2.bilateralFilter(img_gray, 5, 6, 6)\n",
    "            gaussian_blur = cv2.GaussianBlur(bilateral_filtered, (7, 7), 2)\n",
    "            new_image = cv2.addWeighted(bilateral_filtered, 1.5, gaussian_blur, -0.5, 0)\n",
    "            ksize = 30\n",
    "            sigma = 0.2\n",
    "            theta = 0\n",
    "            lambd = 10\n",
    "            gamma = 0.5\n",
    "            gabor_filtered = apply_gabor_filter(new_image, ksize, sigma, theta, lambd, gamma)\n",
    "            # new_image = cv2.resize(gabor_filtered, (850, 850))\n",
    "            gabor_filtered = np.expand_dims(gabor_filtered, axis=-1)\n",
    "            hpp = horizontal_projection_profile(gabor_filtered)\n",
    "            word_line_spaces = word_line_space_formation(hpp)\n",
    "            padded_image = image_padding(gabor_filtered, target_size=(650, 650))\n",
    "            normalized_blocks = block_normalization(padded_image, block_size=(650, 650))\n",
    "            reshaped_blocks = normalized_blocks.reshape(normalized_blocks.shape[1], normalized_blocks.shape[2], normalized_blocks.shape[0]).squeeze()\n",
    "            \n",
    "            \n",
    "            glcm_features = extract_glcms(reshaped_blocks)\n",
    "            features.append(glcm_features)\n",
    "            labels.append(category)  \n",
    "            # new_img_path = os.path.join(destination_folder,img_name)\n",
    "            # cv2.imwrite(new_img_path, gabor_filtered)\n",
    "            # plt.imshow(reshaped_blocks, cmap=\"gray\")\n",
    "            # plt.show()\n",
    "    features = np.array(features)\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    \n",
    "    scaler = MinMaxScaler()\n",
    "    features_normalized = scaler.fit_transform(features)\n",
    "    \n",
    "    return features_normalized, labels, scaler\n",
    "features, labels, scaler  = extract_features()\n",
    "print(\"Shape of features:\", features.shape)\n",
    "print(\"Shape of labels:\", labels.shape)\n",
    "print(\"Labels:\", labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06fc1a18-6b44-4bda-8b92-19af8cc1fe65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.38039455, 0.45977794, 0.49537217, ..., 0.34270042, 0.32684579,\n",
       "        0.37820802],\n",
       "       [0.42169531, 0.44421932, 0.46413582, ..., 0.43410331, 0.43100178,\n",
       "        0.44344155],\n",
       "       [0.34753023, 0.40569835, 0.442048  , ..., 0.38673364, 0.35417323,\n",
       "        0.36803723],\n",
       "       ...,\n",
       "       [0.36558672, 0.37993424, 0.42813865, ..., 0.40096928, 0.3340799 ,\n",
       "        0.31917631],\n",
       "       [0.25044952, 0.32960669, 0.39575376, ..., 0.32983445, 0.21297736,\n",
       "        0.26566279],\n",
       "       [0.18656346, 0.28284959, 0.33476349, ..., 0.40802858, 0.3044716 ,\n",
       "        0.36354297]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "051d2fc4-9aef-4715-adf6-4330709104d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['AMBANI', 'AMBANI', 'AMBANI', ..., 'TOBA', 'TOBA', 'TOBA'],\n",
       "      dtype='<U9')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4032be91-eabd-4f6c-9d92-4a5485e4e3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "integer_encoded = label_encoder.fit_transform(labels)\n",
    "\n",
    "\n",
    "onehot_encoder = OneHotEncoder()\n",
    "integer_encoded = integer_encoded.reshape(len(integer_encoded), 1)\n",
    "one_hot_encoded = onehot_encoder.fit_transform(integer_encoded)\n",
    "\n",
    "\n",
    "labels = one_hot_encoded.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "edf21af8-0e7e-4cb2-9a4d-705c0351d7cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 1.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.],\n",
       "       [0., 0., 0., ..., 0., 0., 1.]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2a8a334-22ac-495e-a394-0a8e46905119",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1297, 8)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b18f35c1-9bdc-4144-be07-c4543b06b3c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1297, 20)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "52502eaa-d3ef-4b43-b6a3-d2c971c80ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7d54356b-f67a-4225-ab27-a0065eb0cd08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train: (1037, 20)\n",
      "Shape of X_test: (260, 20)\n",
      "Shape of y_train: (1037, 8)\n",
      "Shape of y_test: (260, 8)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of X_train:\", X_train.shape)\n",
    "print(\"Shape of X_test:\", X_test.shape)\n",
    "print(\"Shape of y_train:\", y_train.shape)\n",
    "print(\"Shape of y_test:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "27691ca5-3076-47cc-984f-2c7fd8d19b97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train: (1037, 20, 1)\n",
      "Shape of X_test: (260, 20, 1)\n"
     ]
    }
   ],
   "source": [
    "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], X_test.shape[1], 1)\n",
    "\n",
    "print(\"Shape of X_train:\", X_train.shape)\n",
    "print(\"Shape of X_test:\", X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "056595f9-a4f4-42a6-a9a6-5f1465e9cab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input\n",
    "\n",
    "model = Sequential([\n",
    "    Input(shape=(X_train.shape[1], X_train.shape[2])),\n",
    "    Conv1D(32, 3, activation='relu', padding='same'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling1D(2),\n",
    "    Conv1D(64, 3, activation='relu', padding='same'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling1D(2),\n",
    "    Conv1D(128, 3, activation='relu', padding='same'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling1D(2),\n",
    "    Conv1D(128, 3, activation='relu', padding='same'),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling1D(2),\n",
    "    GlobalAveragePooling1D(),\n",
    "    Dense(512, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(8, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f88edfdc-f95a-498a-930a-1a5c88244b15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ conv1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">6,208</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_1                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)               │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv1d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              │          <span style=\"color: #00af00; text-decoration-color: #00af00\">24,704</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_2                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling1d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv1d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              │          <span style=\"color: #00af00; text-decoration-color: #00af00\">49,280</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_3                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling1d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ global_average_pooling1d             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling1D</span>)             │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │          <span style=\"color: #00af00; text-decoration-color: #00af00\">66,048</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 │         <span style=\"color: #00af00; text-decoration-color: #00af00\">131,328</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>)                   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">2,056</span> │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
       "│ conv1d (\u001b[38;5;33mConv1D\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m32\u001b[0m)              │             \u001b[38;5;34m128\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m32\u001b[0m)              │             \u001b[38;5;34m128\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling1d (\u001b[38;5;33mMaxPooling1D\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m32\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv1d_1 (\u001b[38;5;33mConv1D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m64\u001b[0m)              │           \u001b[38;5;34m6,208\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_1                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m64\u001b[0m)              │             \u001b[38;5;34m256\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling1d_1 (\u001b[38;5;33mMaxPooling1D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m64\u001b[0m)               │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv1d_2 (\u001b[38;5;33mConv1D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m128\u001b[0m)              │          \u001b[38;5;34m24,704\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_2                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m128\u001b[0m)              │             \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling1d_2 (\u001b[38;5;33mMaxPooling1D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m128\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ conv1d_3 (\u001b[38;5;33mConv1D\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m128\u001b[0m)              │          \u001b[38;5;34m49,280\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ batch_normalization_3                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m128\u001b[0m)              │             \u001b[38;5;34m512\u001b[0m │\n",
       "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)                 │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ max_pooling1d_3 (\u001b[38;5;33mMaxPooling1D\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m128\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ global_average_pooling1d             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling1D\u001b[0m)             │                             │                 │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 │          \u001b[38;5;34m66,048\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout (\u001b[38;5;33mDropout\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 │         \u001b[38;5;34m131,328\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
       "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m)                   │           \u001b[38;5;34m2,056\u001b[0m │\n",
       "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">281,160</span> (1.07 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m281,160\u001b[0m (1.07 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">280,456</span> (1.07 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m280,456\u001b[0m (1.07 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">704</span> (2.75 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m704\u001b[0m (2.75 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c0850cd2-9c2b-4253-b108-e004aaefc0de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import Adam\n",
    "optimizer = Adam(learning_rate=0.0001)\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "faffaf67-987f-42b9-ba15-bc6bc3e66661",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 56ms/step - accuracy: 0.2033 - loss: 2.2721 - val_accuracy: 0.1683 - val_loss: 2.0844\n",
      "Epoch 2/40\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.4393 - loss: 1.5622 - val_accuracy: 0.1683 - val_loss: 2.0965\n",
      "Epoch 3/40\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - accuracy: 0.5617 - loss: 1.2669 - val_accuracy: 0.1683 - val_loss: 2.1248\n",
      "Epoch 4/40\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.6280 - loss: 1.0858 - val_accuracy: 0.1683 - val_loss: 2.1687\n",
      "Epoch 5/40\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.6827 - loss: 0.9264 - val_accuracy: 0.1587 - val_loss: 2.2335\n",
      "Epoch 6/40\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.7295 - loss: 0.7989 - val_accuracy: 0.1202 - val_loss: 2.3087\n",
      "Epoch 7/40\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.7652 - loss: 0.7169 - val_accuracy: 0.1010 - val_loss: 2.3744\n",
      "Epoch 8/40\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.8029 - loss: 0.5910 - val_accuracy: 0.1250 - val_loss: 2.4670\n",
      "Epoch 9/40\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - accuracy: 0.7859 - loss: 0.6288 - val_accuracy: 0.0769 - val_loss: 2.5892\n",
      "Epoch 10/40\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.8069 - loss: 0.5475 - val_accuracy: 0.0817 - val_loss: 2.6436\n",
      "Epoch 11/40\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - accuracy: 0.8269 - loss: 0.5264 - val_accuracy: 0.1298 - val_loss: 2.6766\n",
      "Epoch 12/40\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.8408 - loss: 0.4786 - val_accuracy: 0.0817 - val_loss: 2.7796\n",
      "Epoch 13/40\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.8654 - loss: 0.4483 - val_accuracy: 0.1490 - val_loss: 2.7500\n",
      "Epoch 14/40\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.8778 - loss: 0.3840 - val_accuracy: 0.1779 - val_loss: 2.6532\n",
      "Epoch 15/40\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - accuracy: 0.8588 - loss: 0.4075 - val_accuracy: 0.1971 - val_loss: 2.6388\n",
      "Epoch 16/40\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 20ms/step - accuracy: 0.8651 - loss: 0.3662 - val_accuracy: 0.2356 - val_loss: 2.5073\n",
      "Epoch 17/40\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.8790 - loss: 0.3749 - val_accuracy: 0.2452 - val_loss: 2.2208\n",
      "Epoch 18/40\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - accuracy: 0.8975 - loss: 0.3249 - val_accuracy: 0.3365 - val_loss: 1.8448\n",
      "Epoch 19/40\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 33ms/step - accuracy: 0.9197 - loss: 0.2689 - val_accuracy: 0.3942 - val_loss: 1.6005\n",
      "Epoch 20/40\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - accuracy: 0.9174 - loss: 0.2794 - val_accuracy: 0.5096 - val_loss: 1.2512\n",
      "Epoch 21/40\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.9088 - loss: 0.2813 - val_accuracy: 0.6298 - val_loss: 1.0303\n",
      "Epoch 22/40\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 31ms/step - accuracy: 0.9207 - loss: 0.2297 - val_accuracy: 0.7163 - val_loss: 0.7318\n",
      "Epoch 23/40\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.9242 - loss: 0.2200 - val_accuracy: 0.7837 - val_loss: 0.6283\n",
      "Epoch 24/40\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.9099 - loss: 0.2587 - val_accuracy: 0.8221 - val_loss: 0.5064\n",
      "Epoch 25/40\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - accuracy: 0.9422 - loss: 0.1832 - val_accuracy: 0.8558 - val_loss: 0.3859\n",
      "Epoch 26/40\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 28ms/step - accuracy: 0.9502 - loss: 0.2007 - val_accuracy: 0.8846 - val_loss: 0.2837\n",
      "Epoch 27/40\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 27ms/step - accuracy: 0.9152 - loss: 0.2451 - val_accuracy: 0.8702 - val_loss: 0.2926\n",
      "Epoch 28/40\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 32ms/step - accuracy: 0.9443 - loss: 0.1861 - val_accuracy: 0.8750 - val_loss: 0.2562\n",
      "Epoch 29/40\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 19ms/step - accuracy: 0.9511 - loss: 0.1981 - val_accuracy: 0.8894 - val_loss: 0.2464\n",
      "Epoch 30/40\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.9458 - loss: 0.1762 - val_accuracy: 0.8942 - val_loss: 0.2412\n",
      "Epoch 31/40\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.9620 - loss: 0.1472 - val_accuracy: 0.9038 - val_loss: 0.2110\n",
      "Epoch 32/40\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 21ms/step - accuracy: 0.9574 - loss: 0.1423 - val_accuracy: 0.9135 - val_loss: 0.2145\n",
      "Epoch 33/40\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.9505 - loss: 0.1811 - val_accuracy: 0.9183 - val_loss: 0.2196\n",
      "Epoch 34/40\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 26ms/step - accuracy: 0.9605 - loss: 0.1378 - val_accuracy: 0.9183 - val_loss: 0.1930\n",
      "Epoch 35/40\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.9555 - loss: 0.1567 - val_accuracy: 0.9231 - val_loss: 0.1791\n",
      "Epoch 36/40\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 24ms/step - accuracy: 0.9677 - loss: 0.1234 - val_accuracy: 0.9375 - val_loss: 0.1668\n",
      "Epoch 37/40\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.9494 - loss: 0.1132 - val_accuracy: 0.9135 - val_loss: 0.1985\n",
      "Epoch 38/40\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 23ms/step - accuracy: 0.9642 - loss: 0.1235 - val_accuracy: 0.9231 - val_loss: 0.1937\n",
      "Epoch 39/40\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9538 - loss: 0.1418 - val_accuracy: 0.9183 - val_loss: 0.2165\n",
      "Epoch 40/40\n",
      "\u001b[1m26/26\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 22ms/step - accuracy: 0.9657 - loss: 0.0970 - val_accuracy: 0.9038 - val_loss: 0.2330\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1f6a8bad9f0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train, \n",
    "batch_size=32, \n",
    "epochs=40, \n",
    "validation_split=0.2,\n",
    "verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e77f615f-2c07-471e-b3ec-4e14c6915e62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - accuracy: 0.9411 - loss: 0.1324\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.14323320984840393, 0.9384615421295166]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c3bd383f-802b-407a-bc71-9a47e449bf4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m9/9\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 53ms/step\n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d1125537-2d93-4440-9285-3ad4518f8726",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_classes = [np.argmax(element) for element in y_pred]\n",
    "y_test_classes = np.argmax(y_test, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "36df646c-de8b-4c8e-a7c1-d25191972e40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.96      0.91        26\n",
      "           1       0.95      1.00      0.97        18\n",
      "           2       0.88      0.97      0.92        29\n",
      "           3       1.00      0.94      0.97        35\n",
      "           4       1.00      0.96      0.98        25\n",
      "           5       0.90      0.97      0.94        39\n",
      "           6       0.98      0.98      0.98        54\n",
      "           7       0.93      0.74      0.82        34\n",
      "\n",
      "    accuracy                           0.94       260\n",
      "   macro avg       0.94      0.94      0.94       260\n",
      "weighted avg       0.94      0.94      0.94       260\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test_classes,y_pred_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "625d3183-40f1-4e91-9c22-7989782207a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(img_path):\n",
    "    img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "    img_array = np.expand_dims(img, axis=-1)\n",
    "    img_size = 850\n",
    "    new_array = cv2.resize(img_array, (img_size, img_size))\n",
    "    img_gray = cv2.adaptiveThreshold(new_array, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 5, 11)\n",
    "    bilateral_filtered = cv2.bilateralFilter(img_gray, 5, 6, 6)\n",
    "    gaussian_blur = cv2.GaussianBlur(bilateral_filtered, (7, 7), 2)\n",
    "    new_image = cv2.addWeighted(bilateral_filtered, 1.5, gaussian_blur, -0.5, 0)\n",
    "    ksize = 30\n",
    "    sigma = 0.2\n",
    "    theta = 0\n",
    "    lambd = 10\n",
    "    gamma = 0.5\n",
    "    gabor_filtered = apply_gabor_filter(new_image, ksize, sigma, theta, lambd, gamma)\n",
    "    gabor_filtered = np.expand_dims(gabor_filtered, axis=-1)\n",
    "    hpp = horizontal_projection_profile(gabor_filtered)\n",
    "    word_line_spaces = word_line_space_formation(hpp)\n",
    "    padded_image = image_padding(gabor_filtered, target_size=(650, 650))\n",
    "    normalized_blocks = block_normalization(padded_image, block_size=(650, 650))\n",
    "    reshaped_blocks = normalized_blocks.reshape(normalized_blocks.shape[1], normalized_blocks.shape[2], normalized_blocks.shape[0]).squeeze()\n",
    "    glcm_features = extract_glcms(reshaped_blocks)\n",
    "    return glcm_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1ee79e30-bce3-483d-abc3-3e672b6c7489",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_prediction(img_path, model, scaler):\n",
    "    features = preprocess_image(img_path)\n",
    "    features_normalized = scaler.transform([features])\n",
    "    prediction = model.predict(features_normalized.reshape(1,20,1))\n",
    "    predicted_class = np.argmax(prediction)\n",
    "    return predicted_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fec8ef89-1225-46b9-92d1-763cff56aed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
      "Prediction: PETER, Confidence: 1.00\n"
     ]
    }
   ],
   "source": [
    "def make_prediction_with_confidence(img_path, model, scaler, label_encoder, threshold=0.4):\n",
    "    \n",
    "    features = preprocess_image(img_path)\n",
    "    features_normalized = scaler.transform([features])\n",
    "    \n",
    "    \n",
    "    prediction = model.predict(features_normalized.reshape(1,-1, 1))\n",
    "    predicted_class = np.argmax(prediction)\n",
    "    confidence = prediction[0][predicted_class]\n",
    "    \n",
    "    \n",
    "    if confidence < threshold:\n",
    "        return \"unrecognized\", confidence\n",
    "    else:\n",
    "        \n",
    "        predicted_label = label_encoder.inverse_transform([predicted_class])[0]\n",
    "        return predicted_label, confidence\n",
    "\n",
    "img_path = r\"C:\\Users\\PC\\Desktop\\PROJECT\\HANDWRITING\\PETER\\pete (52).jpg\"\n",
    "analysis, confidence = make_prediction_with_confidence(img_path, model, scaler, label_encoder, threshold=0.4)\n",
    "\n",
    "print(f\"Prediction: {analysis}, Confidence: {confidence:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e0d2a2f5-840a-40b9-84c3-f0487b7bd967",
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.9.0) :-1: error: (-5:Bad argument) in function 'resize'\n> Overload resolution failed:\n>  - src data type = object is not supported\n>  - Expected Ptr<cv::UMat> for argument 'src'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m img_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mPC\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDesktop\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mPROJECT\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mCELESTINA\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mceles (10).jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# features_normalized, labels, scaler  = extract_features()  \u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# scaler.transform(features_normalized)  \u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m analysis \u001b[38;5;241m=\u001b[39m \u001b[43mmake_prediction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPrediction:\u001b[39m\u001b[38;5;124m\"\u001b[39m, analysis)\n",
      "Cell \u001b[1;32mIn[24], line 2\u001b[0m, in \u001b[0;36mmake_prediction\u001b[1;34m(img_path, model, scaler)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmake_prediction\u001b[39m(img_path, model, scaler):\n\u001b[1;32m----> 2\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m     features_normalized \u001b[38;5;241m=\u001b[39m scaler\u001b[38;5;241m.\u001b[39mtransform([features])\n\u001b[0;32m      4\u001b[0m     prediction \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(features_normalized\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m20\u001b[39m,\u001b[38;5;241m1\u001b[39m))\n",
      "Cell \u001b[1;32mIn[23], line 5\u001b[0m, in \u001b[0;36mpreprocess_image\u001b[1;34m(img_path)\u001b[0m\n\u001b[0;32m      3\u001b[0m img_array \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexpand_dims(img, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m      4\u001b[0m img_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m850\u001b[39m\n\u001b[1;32m----> 5\u001b[0m new_array \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_array\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg_size\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m img_gray \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39madaptiveThreshold(new_array, \u001b[38;5;241m255\u001b[39m, cv2\u001b[38;5;241m.\u001b[39mADAPTIVE_THRESH_GAUSSIAN_C, cv2\u001b[38;5;241m.\u001b[39mTHRESH_BINARY, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m11\u001b[39m)\n\u001b[0;32m      7\u001b[0m bilateral_filtered \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mbilateralFilter(img_gray, \u001b[38;5;241m5\u001b[39m, \u001b[38;5;241m6\u001b[39m, \u001b[38;5;241m6\u001b[39m)\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.9.0) :-1: error: (-5:Bad argument) in function 'resize'\n> Overload resolution failed:\n>  - src data type = object is not supported\n>  - Expected Ptr<cv::UMat> for argument 'src'\n"
     ]
    }
   ],
   "source": [
    "img_path = r\"C:\\Users\\PC\\Desktop\\PROJECT\\test\\CELESTINA\\celes (10).jpg\"\n",
    "# features_normalized, labels, scaler  = extract_features()  \n",
    "# scaler.transform(features_normalized)  \n",
    "analysis = make_prediction(img_path, model, scaler)\n",
    "print(\"Prediction:\", analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "670218dd-292f-4910-892b-3f12221620af",
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis = label_encoder.inverse_transform([analysis])[0]\n",
    "print(\"Prediction:\", analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283a0bf6-a773-4f86-bd94-58509265c178",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925c732e-c650-439d-8571-e5abfe0185b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a18ea61-4873-4031-afef-1775d2dbd7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "726d64b1-c645-447a-967b-606bd851cdb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "model.save(\"handwriting_recognition_model.keras\")\n",
    "\n",
    "with open(\"label_encoder.pkl\", \"wb\") as le_file:\n",
    "    \n",
    "    joblib.dump(label_encoder, le_file)\n",
    "with open(\"scaler.pkl\", \"wb\") as scaler_file:\n",
    "    joblib.dump(scaler, scaler_file)\n",
    "print(\"saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "e1f29cd8-76cc-4631-b19f-ca7004fbd4e3",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "(unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \\UXXXXXXXX escape (2920531907.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[81], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    \"C:\\Users\\PC\\Desktop\\PROJECT\\unrecognized\\unrecognized (6).jpg\"\u001b[0m\n\u001b[1;37m                                                                   ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m (unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \\UXXXXXXXX escape\n"
     ]
    }
   ],
   "source": [
    "\"C:\\Users\\PC\\Desktop\\PROJECT\\unrecognized\\unrecognized (6).jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ccac4f-dedb-4405-adee-a46e4b288d38",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
